{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.corpus import Corpus\n",
    "from itertools import combinations\n",
    "from utils import *\n",
    "\n",
    "subset = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change cwd\n",
    "os.chdir(\"/Users/emiltrencknerjessen/Desktop/priv/DANSK-gold-NER\")\n",
    "\n",
    "# Load language object\n",
    "nlp = spacy.blank(\"da\")\n",
    "#nlp = dacy.load('medium')\n",
    "\n",
    "# List relevant data and sort by rater number\n",
    "data_paths = glob.glob(\"./data/multi/unprocessed/rater*/data.spacy\")\n",
    "data_paths.sort()\n",
    "data_paths.sort(key=\"./data/multi/unprocessed/rater_10/data.spacy\".__eq__)\n",
    "\n",
    "# Load in data and get rater indices (if not already loaded)\n",
    "data = []\n",
    "raters_idx = []\n",
    "for path in data_paths:\n",
    "    # Get rater indices\n",
    "    rater_idx = int(re.search(r\"\\d+\", path).group()) -1\n",
    "    raters_idx.append(rater_idx)\n",
    "    \n",
    "    # Load data\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    if subset:\n",
    "        docs = list(doc_bin.get_docs(nlp.vocab))[:10]\n",
    "    else:\n",
    "        docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "    data.append(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excluding rater 2, and 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indexes for raters to exclude\n",
    "indexes = [1, 9]\n",
    "\n",
    "# For each index in reverse order, remove them from the data\n",
    "for index in sorted(indexes, reverse=True):\n",
    "    del data[index]\n",
    "\n",
    "# Update raters indexes\n",
    "raters_idx = raters_idx[:8]\n",
    "\n",
    "# Have a lookup table for the index vs. rater number.\n",
    "raters_lookup = {0: 1, 1: 3, 2: 4, 3: 5, 4: 6, 5: 7, 6: 8, 7: 9}\n",
    "\n",
    "# Keys = index\n",
    "# Value = rater"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve unique documents and flat list with all docs from all raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of all unique docs\n",
    "# unique_docs_texts = []\n",
    "# unique_docs_idx_in_all_docs = []\n",
    "# all_docs = [item for sublist in data for item in sublist]\n",
    "# for idx, doc in enumerate(all_docs):\n",
    "#     print(f\"doc with idx {idx} out of {len(all_docs)-1} indices\")\n",
    "#     if doc.text not in unique_docs_texts:\n",
    "#         unique_docs_texts.append(doc.text)\n",
    "#         unique_docs_idx_in_all_docs.append(idx)\n",
    "# unique_docs = list(itemgetter(*unique_docs_idx_in_all_docs)(all_docs))\n",
    "\n",
    "# all_docs = [item for sublist in data for item in sublist]\n",
    "\n",
    "\n",
    "# Get a list with all docs from all raters (including duplicate docs)\n",
    "all_docs = [item for sublist in data for item in sublist]\n",
    "\n",
    "# Get a list of all unique docs\n",
    "unique_docs = []\n",
    "for doc in all_docs:\n",
    "    if all(doc.text != unique_doc.text for unique_doc in unique_docs):\n",
    "        unique_docs.append(copy.deepcopy(doc))\n",
    "\n",
    "# Ensure that unique_docs don't already have entities\n",
    "for i in unique_docs:\n",
    "    i.ents = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_freq = 0\n",
    "# threshold_infreq = 0\n",
    "# doc = data[2][276]\n",
    "# rater_docs = data[2]\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_ents_full_match,unique_ents_partial_match,freq_unique_ents_full_match,infreq_unique_ents_partial_match,unique_ents_full_match_count,unique_ents_partial_match_count, n_raters = retrieve_freq_and_infreq_ents_from_doc(doc, all_docs, threshold_freq, threshold_infreq)\n",
    "# streamline_doc(doc, rater_docs, freq_unique_ents_full_match, infreq_unique_ents_partial_match)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline all docs for all raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDoc does not exist for rater\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     unique_ents_full_match,unique_ents_partial_match,freq_unique_ents_full_match,infreq_unique_ents_partial_match,unique_ents_full_match_count,unique_ents_partial_match_count, n_raters \u001b[39m=\u001b[39m retrieve_freq_and_infreq_ents_from_doc(doc, all_docs, threshold_freq, threshold_infreq)\n\u001b[1;32m     15\u001b[0m     streamlined_doc \u001b[39m=\u001b[39m streamline_doc(doc, rater_docs, freq_unique_ents_full_match, infreq_unique_ents_partial_match)\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCurrent rater: \u001b[39m\u001b[39m{\u001b[39;00mraters_lookup[rater_idx]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 6)"
     ]
    }
   ],
   "source": [
    "threshold_freq = .1\n",
    "threshold_infreq = .075\n",
    "n_raters = len(raters_idx)\n",
    "\n",
    "streamlined_data = []\n",
    "\n",
    "for rater_idx in raters_idx:\n",
    "    streamlined_rater_docs = []\n",
    "    rater_docs = copy.deepcopy(data[rater_idx])\n",
    "    for doc in unique_docs:\n",
    "        if get_same_doc_index(doc, rater_docs) is None: \n",
    "            print('Doc does not exist for rater')\n",
    "        else:\n",
    "            unique_ents_full_match,unique_ents_partial_match,freq_unique_ents_full_match,infreq_unique_ents_partial_match,unique_ents_full_match_count,unique_ents_partial_match_count,n_raters = retrieve_freq_and_infreq_ents_from_doc(doc, all_docs, threshold_freq, threshold_infreq)\n",
    "            streamlined_doc = streamline_doc(doc, rater_docs, freq_unique_ents_full_match, infreq_unique_ents_partial_match)\n",
    "            print(f'Current rater: {raters_lookup[rater_idx]}')\n",
    "            print(f'Current rater_idx: {rater_idx}')\n",
    "            print(f'N_raters for doc: {n_raters}')\n",
    "\n",
    "            \n",
    "            print(f'Current doc index in rater: {get_same_doc_index(doc, rater_docs)}')\n",
    "            print(f'Current doc: {doc}')\n",
    "            \n",
    "            print(f'Unique_ents_full: {unique_ents_full_match}')\n",
    "            print(f'Unique_ents_full count: {unique_ents_full_match_count}')\n",
    "            print(f'Freq ents (no duplicates): {freq_unique_ents_full_match}')\n",
    "            \n",
    "            print(f'Unique_ents_partial: {unique_ents_partial_match}')\n",
    "            print(f'Unique_ents_partial count: {unique_ents_partial_match_count}')\n",
    "            print(f'Infreq ents (no overlaps): {infreq_unique_ents_partial_match}')\n",
    "            \n",
    "            print(f'Current doc ents PRIOR to streamlining: {rater_docs[get_same_doc_index(doc, rater_docs)].ents}')\n",
    "            print(f'Current doc ents AFTER streamlining: {streamlined_doc.ents}')\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "        if streamlined_doc != None:\n",
    "            streamlined_rater_docs.append(streamlined_doc)\n",
    "    streamlined_data.append(streamlined_rater_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all streamlined docs as DocBins (jsonl??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all streamlined docs as .jsonl\n",
    "for rater_idx in raters_idx:\n",
    "    db = DocBin()\n",
    "    #savepath = f\"./data/multi/streamlined/rater_{raters_lookup[rater_idx]}/train.jsonl\"\n",
    "    savepath = f\"./data/multi/streamlined/rater_{raters_lookup[rater_idx]}/train.spacy\"\n",
    "    for doc in streamlined_data[rater_idx]:\n",
    "        db.add(doc)\n",
    "    db.to_disk(savepath)\n",
    "    # examples = []\n",
    "    # for doc in db.get_docs(nlp.vocab):\n",
    "    #     spans = [{\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_} for ent in doc.ents]\n",
    "    #     examples.append({\"text\":doc.text,\"spans\":spans,\"_view_id\": \"ner_manual\"})\n",
    "    # with open(savepath, 'w') as outfile:\n",
    "    #     for entry in examples:\n",
    "    #         json.dump(entry, outfile)\n",
    "    #         outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f092f9808d781aed1cffbee1778720285e402013b75f9f1b9e21e42cac7a28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
